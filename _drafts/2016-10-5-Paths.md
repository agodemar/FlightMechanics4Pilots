---
layout: post
title: The path is the way
category: notes
---

What can we find and where do they take us?

# Linear paths

Linear paths are the nice ones, no nasty bumps in the way...

### Matrix multiplication

So it turns out that matrix multiplacations naturally capture paths. They can be ...

$$
\begin{align*}
y &= ABx \\
&= \begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix}
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22} \\
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
\end{bmatrix} \\
&=\begin{bmatrix}
a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
\end{bmatrix}
\end{align*}
$$

![]({{ site.baseurl }}/images/LinearPath.png){: .center-image }

So as we can see, the top left entry of $AB_{1,1}=a_{11}b_{11} + a_{12}b_{21}$ is the sum of the two paths shown in orange.

### Related algorithms

Interesting similarities to graph algoriths.
Sum-product. Verterbi?
Message passing.

# Non-linearities

Kawaguchi/Choromasnska make bad assumptions. That paths are independent, and ??

$$
y_{j,i} = \sum_{p=1}^{\mid P_j\mid} [X_i]_{(j,p)} [Z_i]_{(j,p)} \prod_{k=1}^{H+1} w_{k,p}^{(k)} \\
$$

Consider. We if we know that ????
Entangled! $P(p_1,p_2) = P(p_1)P(p_2)$ in the linear case. But ... (want a nice graphical model pic? like explaining away?)



# Conjecture

1. __Deep paths__
    * Increases in the depth of paths allow them to make higher order frequencies/oscillations.
    * Higher order frequencies/oscillations give increased representational power/accuracy.
    * Increased representational power gives greater efficiency in number of paths required, O(k^n) vs O(n^k).
    * Deep nets have deeper paths than wide nets.
2. __Weight tying across paths__
    * Deep nets have exponentially more paths than wide nets.
    * Nets share weights in ?? ways
    * Weights with more shared paths can be learned faster. Weight tying across paths gives 'more information' to each weight (as each weight is used a bunch of times).
    * 'More information' means faster training/easier learning.


Therefore: Deep networks are better than wide nets. They can learn more complex functions, in O(? size of net) vs O(? size of net), and can learn them faster, O(?? t) vs O(?? t) .


# Quantum interpretation?
