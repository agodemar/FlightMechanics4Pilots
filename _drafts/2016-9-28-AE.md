---
layout: post
title: Autoencoders
---

One of the main reason I like autoencoders is that they are simple and recursive.


<!--
(how am I going to make this post fun?!?)
A nice metaphor or a person trying to reconstruct paintings after their child has ?? them
Linear = constrained ability to mix paints
Stable = tracing?
Generative = he's a painter/natural. his sneezes make art...
Spare = taking photo/using a poster?
-->


<!--
what point am I making? and why bother?
- a review of AEs
- that i am smart and know shit...
- what do we want to know about AEs?
-
-->

# Autoencoders


$$
\begin{align*}
\mathcal L = \sum_{t=0}^T \parallel x_t - D(E(x_t)) \parallel_2^2 \tag{vanilla}\\
\end{align*}
$$

### Variants

##### Linear

$$
\begin{align}
E(z) = Bz \;, D(z) = Az \\
\mathcal L = \sum_{t=0}^T \parallel y_t - ABx_t \parallel^2_2 \tag{y=x}
\end{align}
$$

For a linear AE we can show that each critical point of $\mathcal L(A,B)$ is some combination of principle components of $\Sigma = y^Tx$. And that the minium loss is achieved when the largest eigenvalues/leading eigenvectors are chosen/learnt.


$$
\begin{align}
A^T(AB - \Sigma_{YX}\Sigma^{-1}_{XX}) = 0 \\
S = AB - P_A \Sigma_{YX}\Sigma^{-1}_{XX} \\
\end{align}
$$

To show (1) they set A and solve for B given $E(A,B)$ is at a critical point (aka that the gradient of loss is zero) -- and vice versa. Using this result we find that AB is equal to the projection of the covariance YX and the inverse of covariance XX into a subspace of A (using a clever argument to show that $S = 0$, as S is both in the subspace of A and orthogonal to A). It follows that $\Sigma$ is invariant to the columns of A, which is (roughly) the definition of an eigen vector.

To show (2) we use a unitary change of coordinates, where the covariance XX is diagonal, to allow some nice rearranging. Because AB is a projection matrix we know that we can drop the aquared term, thus we can cancel we can simplity to the trace of $tr(\Sigma) - tr(AB\Sigma)$. Then using pertubation analysis of ...

$$
\begin{align}
P_C &= C(C^TC)^{-1}C^T\\
C &= U\Lambda V^T \tag{SVD}\\
\implies P_C &=(U\Lambda V^T) \big( (U\Lambda V^T)^T (U\Lambda V^T)\big)^{-1}(U\Lambda V^T)^T \\
&=U\Lambda V^T \big(V\Lambda U^T U\Lambda V^T)^{-1} V\Lambda^T U^T \\
&=U\Lambda V^T V\Lambda^{-2} V^T V\Lambda^T U^T \\
&=U \Lambda \Lambda^{-2} \Lambda^T U^T \\
\end{align}
$$



* We did second order optimisation without computing the hessian.?!?
* What was the most important part of this paper? How significant is that that we could describe the loss surface? Why is this important?
[Neural Networks and PCA](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.408.1839&rep=rep1&type=pdf) [Complex valued autoencoders](http://arxiv.org/abs/1108.4135)


Summary of Kawaguchi
[Deep learning without local minima](http://arxiv.org/abs/1605.07110)

$$
\begin{align*}
M &=\begin{bmatrix}
A & B \\
B^T & C \\
\end{bmatrix} \\
M &\ge 0 \tag{if this then ...}\\
& \implies A\ge 0 \\
& \implies \mathcal R(B) \subseteq \mathcal R(A) \\
& \implies C - BA^-B^T \ge 0 \\
\end{align*}
$$


Some words about why the generalised inverses are hard?!? Why all the fuss with schur compliments?
Note: Reparameterisation breaks ???
Thoughts about one global minima vs multiple distinct ones. Can you ctsly transform between them? (maybe a picture and a proof?)



##### Stable
$$
\begin{align*}
\mathcal L &= \parallel x - D(E(x + \epsilon)) \parallel_2^2 \tag{denoising}\\
\mathcal L &= \parallel x - D(E(x)) \parallel_2^2 + \parallel \frac{\partial E(x)}{\partial x} \parallel_F^2 \tag{contractive}\\
\end{align*}
$$

[Contractive Auto-Encoders](http://www.icml-2011.org/papers/455_icmlpaper.pdf)
I think the connection between gaussian noise and regulaisation of gradients is quite important!! (this hints at how SGD can be used to get second order information)


##### Generative

$$
\begin{align*}
\mathcal L = -D_{KL}(q_{\phi }(z \mid x ) \parallel p_{\theta }(z )) + E_{q_{\phi }(z \mid x )} \big( \log p_{\theta }(x \mid z ) \big) \tag{variational}\\
\end{align*}
$$
[Autoencoding variational bayes](https://arxiv.org/abs/1312.6114)


$$
\begin{align*}
\mathcal L_{D,E} &= \sum_{t=0}^T \parallel x_t - D(E(x_t)) \parallel_2^2  \tag{adversarial}\\
\mathcal L_D &= -\log(1-C(D(z))) - \log(C(x)) \tag{decoder = generator}\\
\mathcal L_C &= -\log(C(D(z))) \tag{discriminator}\\
\end{align*}
$$
GANs relation to JSD?
[Adversarial autoencoders](https://arxiv.org/abs/1511.05644)

AdvAE = learning a distance metric/lossfunction

##### Sparse

$$
\begin{align*}
\mathcal L = \parallel x - D(E(x)) \parallel_2^2 + \lambda \parallel E(x) \parallel_1 \tag{sparse}\\
\end{align*}
$$

(what other types of regularisation could/should be used?)

***

* Convolutional and recurrent = ?
* Sigmoid = same energy function as deep belief nets?
*




### Pathologies

* Mean output
* Weight sharing does worse ?!!?
* Reconstructing unseen data
*


### ??

* as compressors
* communication through code space? cryptography??
*

***
See [here]() for an implementation of each autoencoder. (a nice simple gist where you can just change the loss fn)
So, more generally, how can we intelligently pick supervision schemes/loss functions that allow us to learn ???.
(this might have to be another post.. Learning from context)
