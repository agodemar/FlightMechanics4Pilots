---
layout: post
title: Softmax approximation and candidate sampling
category: summary
---

According to [???]() softmax is them main computational cost
(should show this? = $\mathcal O(?)$ and some empirical tests?).


This problem is especially important in setting with many labels. E.g.
* pixelwise attention

<!-- ####################################################################### -->

## What do we really want?

What is `softmax` for?

A function that;
* approaches `max`?
* that is differentiable?
* that normalises log probabilities?
*


<!-- ####################################################################### -->
# Questions

* How are the probabilities distributed? As a power law?
* What if we wanted sparse distributions?
* 


<!-- ####################################################################### -->
# Other thoughts

### Delta update rule

Results in the same update rule.

$$\frac{\partial \mathcal L}{\partial \theta} = x \cdot (T - y)$$
Which is very close to hebbian ($xT$) and anti-hebbian ($-xy$) learning.

### Softmax + cross entropy = exponentiated gradients?

Proof from online mirror descent?

###

<!-- ####################################################################### -->
## Related ideas and rescources

Softmin.
?


* [Sebastian Ruder's blog](http://sebastianruder.com/word-embeddings-softmax/)
* [Efficient softmax approximation for GPUs](https://arxiv.org/abs/1609.04309)
* https://www.tensorflow.org/extras/candidate_sampling.pdf
