---
layout: post
title: Calculus
---

[The essence of calculus]({{ baseurl }}/EssenceofCalc)
[Motivating calculus]({{ baseurl }}/MotivationofCalc)

Decomposing hard problems into an approximation via a large sums of distinct parts. But can then efficiently evaluate the approximation by taking a limit.
Linearization!!
Continuious!!


## Differences

### Finite differences

Want to

Alternative derivation/framing?

Connected neighbors. Approximation given local behaviour.

[intuition]({{ baseurl }}/)

which leads to empirical calculation of gradients.


### Temporal differences.

Credit assignment in ML.
How is it used. What does it do. Local credit assignment.
Making it efficient? AD?

## Automatic differentiation

Forward and reverse AD.
Relation to symbolic gradients.
Relation to duals.([doubly recursive gist]({{ gist }}/recursiveAD) and )

[minimal implementation of reverse AD on graphs]({{ gist }}/simpleAD)
Incremental lambda calculus.

## Non-differentiable

### Solutions. GA, RL.

Problem Credit assignment

### Settings

* discrete (soft approximations)
* random (mean field).
* unknown (Balduzzi on approximation gradient fns)



solutions?

concrete distribution. ...
MCMC of counterfactuals?

***
Build around the idea that differentiation is a linear operator and linear operators duality as a function and a transform.

$$
\mathcal D x=\frac{\partial f(x)}{\partial x} \
$$



* https://rip94550.wordpress.com/2012/08/27/heavisides-operational-calculus/
* https://arxiv.org/abs/1610.07690
